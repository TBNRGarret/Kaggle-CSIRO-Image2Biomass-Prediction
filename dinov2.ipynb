{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d06554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# =============================================================================\n",
    "class CFG:\n",
    "    # --- Paths ---\n",
    "    BASE_PATH = \"/content/dataset\"\n",
    "    train_csv = os.path.join(BASE_PATH, \"train.csv\")\n",
    "\n",
    "    # --- Model: DINOv2 Giant (Cá»±c náº·ng -> Cáº©n tháº­n Batch Size) ---\n",
    "    model_name = \"vit_giant_patch14_reg4_dinov2\"\n",
    "\n",
    "    # --- Input Size ---\n",
    "    # LÆ°u Ã½: 756x1512 lÃ  ráº¥t lá»›n. Náº¿u OOM (trÃ n RAM) thÃ¬ giáº£m xuá»‘ng (518, 1036)\n",
    "    img_size = (756, 1512)\n",
    "\n",
    "    # --- Training Params ---\n",
    "    batch_size = 4\n",
    "    accumulate_grad = 2   # TÃ­ch lÅ©y gradient\n",
    "    epochs = 20\n",
    "    FREEZE_EPOCHS = 3\n",
    "\n",
    "    lr_backbone = 1e-6    # Giant model cáº§n learning rate ráº¥t nhá»\n",
    "    lr_head = 1e-4\n",
    "    weight_decay = 1e-4\n",
    "    seed = 42\n",
    "    n_fold = 5\n",
    "\n",
    "    num_workers = 4       # TÄƒng lÃªn Ä‘á»ƒ load áº£nh nhanh hÆ¡n\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Thá»© tá»± target pháº£i khá»›p vá»›i logic loss: [Green, Dead, Clover, GDM, Total]\n",
    "    target_cols = [\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA PREPROCESSING\n",
    "# =============================================================================\n",
    "def get_processed_train_df(csv_path: str) -> pd.DataFrame:\n",
    "    print(f\"Loading data from: {csv_path}\")\n",
    "    df_long = pd.read_csv(csv_path)\n",
    "\n",
    "    df_long[\"image_path\"] = df_long[\"image_path\"].astype(str)\n",
    "    df_long[\"image_path_full\"] = df_long[\"image_path\"].apply(\n",
    "        lambda x: os.path.join(CFG.BASE_PATH, x)\n",
    "    )\n",
    "\n",
    "    meta_cols = [\"image_path\", \"image_path_full\", \"Sampling_Date\", \"State\", \"Species\", \"Pre_GSHH_NDVI\", \"Height_Ave_cm\"]\n",
    "    print(\"ðŸ” Pivot Long â†’ Wide...\")\n",
    "    df_wide = df_long.pivot_table(index=meta_cols, columns=\"target_name\", values=\"target\").reset_index()\n",
    "    df_wide.columns.name = None\n",
    "    df_wide = df_wide.fillna(0.0)\n",
    "\n",
    "    for col in CFG.target_cols:\n",
    "        if col not in df_wide.columns: df_wide[col] = 0.0\n",
    "\n",
    "    df_wide[\"group\"] = df_wide[\"Sampling_Date\"].astype(str)\n",
    "    le = LabelEncoder()\n",
    "    df_wide[\"stratify_col\"] = le.fit_transform(df_wide[\"State\"].astype(str))\n",
    "\n",
    "    df_wide[\"fold\"] = -1\n",
    "    sgkf = StratifiedGroupKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "\n",
    "    for fold, (_, val_idx) in enumerate(sgkf.split(df_wide, df_wide[\"stratify_col\"], groups=df_wide[\"group\"])):\n",
    "        df_wide.loc[val_idx, \"fold\"] = fold\n",
    "\n",
    "    print(f\"âœ… Data processed! Shape: {df_wide.shape}\")\n",
    "    return df_wide\n",
    "\n",
    "# =============================================================================\n",
    "# 3. DATASET\n",
    "# =============================================================================\n",
    "class BiomassDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.paths = df['image_path_full'].values\n",
    "        self.targets = df[CFG.target_cols].values.astype(np.float32)\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            img = np.zeros((CFG.img_size[0], CFG.img_size[1], 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, torch.tensor(self.targets[idx])\n",
    "\n",
    "def get_transforms(data='train'):\n",
    "    if data == 'train':\n",
    "        return A.Compose([\n",
    "            A.Resize(CFG.img_size[0], CFG.img_size[1]),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, p=0.5), # TÄƒng cÆ°á»ng augmentation\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(CFG.img_size[0], CFG.img_size[1]),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "# =============================================================================\n",
    "# 4. CUSTOM PHYSICS-INFORMED LOSS\n",
    "# =============================================================================\n",
    "class CSIRO_ConsistencyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.L1Loss() # DÃ¹ng MAE Loss cho robust vá»›i outlier\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        # targets columns: [0:Green, 1:Dead, 2:Clover, 3:GDM, 4:Total]\n",
    "\n",
    "        # 1. Component Loss: Dá»± Ä‘oÃ¡n Ä‘Ãºng tá»«ng thÃ nh pháº§n\n",
    "        loss_components = self.l1(preds, targets)\n",
    "\n",
    "        # 2. Physics Constraint: Green + Dead + Clover + GDM = Total\n",
    "        # Láº¥y tá»•ng cÃ¡c thÃ nh pháº§n con (4 cá»™t Ä‘áº§u)\n",
    "        pred_sum_components = torch.sum(preds[:, :4], dim=1)\n",
    "        pred_total_label = preds[:, 4]\n",
    "\n",
    "        # Pháº¡t náº¿u tá»•ng con khÃ´ng báº±ng tá»•ng cha\n",
    "        loss_constraint = self.l1(pred_sum_components, pred_total_label)\n",
    "\n",
    "        # Tá»•ng há»£p (Æ¯u tiÃªn logic toÃ¡n há»c)\n",
    "        return loss_components + 0.5 * loss_constraint\n",
    "\n",
    "# =============================================================================\n",
    "# 5. MODEL\n",
    "# =============================================================================\n",
    "class BiomassModel(nn.Module):\n",
    "    def __init__(self, model_name, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=0,\n",
    "            global_pool=\"token\",\n",
    "            dynamic_img_size=True\n",
    "        )\n",
    "        # QUAN TRá»ŒNG: KÃ­ch hoáº¡t Checkpointing Ä‘á»ƒ khÃ´ng OOM vá»›i Giant Model\n",
    "        self.backbone.set_grad_checkpointing(True)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.backbone.num_features, 1024),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "# =============================================================================\n",
    "# 6. TRAINING LOOP\n",
    "# =============================================================================\n",
    "def train_loop(fold_idx=0):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"ðŸš€ START TRAINING FOLD {fold_idx} | DINOv2 GIANT\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    # 1. Data Setup\n",
    "    df = get_processed_train_df(CFG.train_csv)\n",
    "    train_df = df[df['fold'] != fold_idx].reset_index(drop=True)\n",
    "    valid_df = df[df['fold'] == fold_idx].reset_index(drop=True)\n",
    "    del df; gc.collect()\n",
    "\n",
    "    train_loader = DataLoader(BiomassDataset(train_df, get_transforms('train')),\n",
    "                              batch_size=CFG.batch_size, shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(BiomassDataset(valid_df, get_transforms('valid')),\n",
    "                              batch_size=CFG.batch_size, shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True)\n",
    "\n",
    "    # 2. Init Model\n",
    "    model = BiomassModel(CFG.model_name, pretrained=True).to(CFG.device)\n",
    "\n",
    "    # --- Phase 1: Freeze Backbone ---\n",
    "    print(f\"ðŸ§Š Phase 1: Freezing Backbone ({CFG.FREEZE_EPOCHS} epochs)\")\n",
    "    for param in model.backbone.parameters(): param.requires_grad = False\n",
    "\n",
    "    optimizer = AdamW(model.head.parameters(), lr=CFG.lr_head, weight_decay=CFG.weight_decay)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.epochs, eta_min=1e-7)\n",
    "\n",
    "    # Thay Loss thÆ°á»ng báº±ng Physics Loss\n",
    "    criterion = CSIRO_ConsistencyLoss()\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    best_mae = float('inf') # ÄÃ¡nh giÃ¡ báº±ng MAE thay vÃ¬ Loss\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        # --- Phase 2: Unfreeze Backbone ---\n",
    "        if epoch == CFG.FREEZE_EPOCHS:\n",
    "            print(f\"ðŸ”¥ Phase 2: Unfreezing Backbone! Switching to small LR.\")\n",
    "            for param in model.backbone.parameters(): param.requires_grad = True\n",
    "\n",
    "            # Re-init Optimizer\n",
    "            optimizer = AdamW([\n",
    "                {'params': model.backbone.parameters(), 'lr': CFG.lr_backbone},\n",
    "                {'params': model.head.parameters(), 'lr': CFG.lr_head}\n",
    "            ], weight_decay=CFG.weight_decay)\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.epochs - epoch, eta_min=1e-7)\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        pbar = tqdm(train_loader, desc=f\"Ep {epoch+1}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        for step, (imgs, targets) in enumerate(pbar):\n",
    "            imgs, targets = imgs.to(CFG.device), targets.to(CFG.device)\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss = loss / CFG.accumulate_grad # Normalize loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % CFG.accumulate_grad == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_losses.append(loss.item() * CFG.accumulate_grad)\n",
    "            pbar.set_postfix(loss=loss.item() * CFG.accumulate_grad)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation (Calculate MAE)\n",
    "        model.eval()\n",
    "        val_preds, val_targets_all = [], []\n",
    "        with torch.no_grad():\n",
    "            for imgs, targets in valid_loader:\n",
    "                imgs = imgs.to(CFG.device)\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    outputs = model(imgs)\n",
    "\n",
    "                val_preds.append(outputs.float().cpu().numpy())\n",
    "                val_targets_all.append(targets.float().numpy())\n",
    "\n",
    "        val_preds = np.concatenate(val_preds)\n",
    "        val_targets_all = np.concatenate(val_targets_all)\n",
    "\n",
    "        # TÃ­nh MAE cho tá»«ng cá»™t\n",
    "        mae_scores = []\n",
    "        for i, col in enumerate(CFG.target_cols):\n",
    "            mae = mean_absolute_error(val_targets_all[:, i], val_preds[:, i])\n",
    "            mae_scores.append(mae)\n",
    "\n",
    "        avg_mae = np.mean(mae_scores)\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "\n",
    "        print(f\"Ep {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val MAE (Avg): {avg_mae:.2f}g\")\n",
    "        print(f\" > Details: Green:{mae_scores[0]:.1f}, Dead:{mae_scores[1]:.1f}, Total:{mae_scores[4]:.1f}\")\n",
    "\n",
    "        if avg_mae < best_mae:\n",
    "            best_mae = avg_mae\n",
    "            torch.save(model.state_dict(), f\"best_fold{fold_idx}.pt\")\n",
    "            print(f\"ðŸŽ‰ Saved Best Model! MAE: {best_mae:.2f}g\")\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if os.path.exists(CFG.train_csv):\n",
    "        train_loop(fold_idx=0)\n",
    "    else:\n",
    "        print(f\"âŒ Check path: {CFG.train_csv}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
